{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview:\n",
    "\n",
    "This utility script provides a streamlined way to process and load large volumes of financial data into a ScyllaDB database. It primarily targets minute-resolution, adjusted-split financial bars but can be adapted for different data structures. In addition to its core functionalities, it offers the `fetch_minute_data` function to retrieve specific minute-level datasets from the database. The script uses efficient techniques such as multiprocessing, batching, and asynchronous fetching to enhance speed and consistency.\n",
    "\n",
    "---\n",
    "\n",
    "## Features:\n",
    "\n",
    "- **Parallel Processing:** Utilizes Python's multiprocessing library to parallelize data processing, making full use of available CPU cores.\n",
    "- **Automatic Retries:** Implements a retry mechanism for data insertions, ensuring data consistency even in the face of transient database errors.\n",
    "- **Progress Monitoring:** Offers a real-time progress bar to keep track of the data loading process.\n",
    "- **Error Logging:** Captures and logs errors encountered during data processing and loading for easier troubleshooting.\n",
    "- **Data Preprocessing:** Preprocesses raw data files to extract relevant information and organize it into a more database-friendly format.\n",
    "- **Custom Date Ranges:** Users can set specific start and end times using `fetch_minute_data`.\n",
    "- **Trading Hours Filter:** Provides an option to retrieve data only from standard trading hours.\n",
    "- **Month-based Bucketing System:** Organizes data for efficient retrieval.\n",
    "- **Asynchronous Data Fetching:** Enables parallel data retrieval across different time intervals.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup and Usage:\n",
    "\n",
    "### Prerequisites:\n",
    "- Linux\n",
    "- Python 3.11.4\n",
    "- ScyllaDB\n",
    "- Python packages: `os`, `shutil`, `subprocess`, `multiprocessing`, `cassandra-driver`, `tqdm`, `datetime`\n",
    "\n",
    "### Configuration:\n",
    "1. Adjust the constants at the beginning of the script, such as `KEYSPACE`, `TABLE`, and `CSV_PATH`, to match your setup.\n",
    "2. Ensure the ScyllaDB cluster is up and accessible from the script execution environment.\n",
    "\n",
    "### Execution:\n",
    "- Simply run the cells sequentially. By default, the script processes a specific number of .txt files located in the directory specified by `CSV_PATH`. Adjust as needed for processing different file counts.\n",
    "\n",
    "---\n",
    "\n",
    "## Function Descriptions:\n",
    "\n",
    "- `divide_list_into_chunks()`: Splits a list into approximately equal-sized chunks for parallel processing.\n",
    "- `list_files()`: Lists all '.txt' files in the specified directory.\n",
    "- `create_keyspace_and_table()`: Establishes a connection to ScyllaDB and creates a keyspace and table if not already present.\n",
    "- `clear_temp_folder()`: Clears any existing temporary files before processing starts.\n",
    "- `log_error()`: Logs errors encountered during data processing.\n",
    "- `execute_with_retry()`: Retries data insertion in case of transient database errors.\n",
    "- `get_bucket_from_timestamp()`: Extracts year-month info from timestamps, useful for bucketing data.\n",
    "- `preprocess_and_load()`: Processes each financial data file and loads the data into ScyllaDB.\n",
    "- `process_chunk()`: Handles a chunk of data files, invoking preprocessing and loading for each.\n",
    "- `monitor_progress()`: A thread-safe function that updates the progress bar as data files are processed.\n",
    "- `fetch_minute_data`: Retrieves minute-level financial data from the database based on specified criteria.\n",
    "\n",
    "---\n",
    "\n",
    "### XFS Filesystem:\n",
    "\n",
    "ScyllaDB, being a high-performance NoSQL database, has specific requirements when it comes to disk I/O. It's crucial to choose the right filesystem to achieve optimal performance. Here's why XFS is recommended for ScyllaDB:\n",
    "\n",
    "- **High Throughput:** XFS is designed for high parallelism, which is in line with ScyllaDB's architecture. This ensures the database can manage multiple read/write operations efficiently.\n",
    "\n",
    "- **Delayed Allocation:** XFS improves disk performance with its delayed allocation feature, helping in reducing fragmentation.\n",
    "\n",
    "- **Scalability:** XFS supports filesystems up to 8 exabytes, which makes it suitable for ScyllaDB deployments that might need to handle large volumes of data.\n",
    "\n",
    "- **Journaling Capability:** XFS has a robust journaling mechanism that ensures data integrity even in case of system crashes. This feature is crucial for databases like ScyllaDB, where data integrity is paramount.\n",
    "\n",
    "- **Optimized for Large Files:** ScyllaDB often deals with large SSTable files. XFS's design is optimized for handling large files, ensuring quick reads and writes.\n",
    "\n",
    "- **Built-in Utilities:** XFS comes with a set of administrative utilities like `xfs_repair` (for repairing the filesystem) and `xfs_growfs` (for resizing the filesystem), which can be beneficial for database management.\n",
    "\n",
    "When deploying ScyllaDB, using the XFS filesystem ensures you are aligning with ScyllaDB’s design principles and maximizing the database's performance capabilities.\n",
    "\n",
    "**Note:** Always ensure you have backups of your data and that you've set up appropriate monitoring and alerting for your ScyllaDB cluster. Regularly check the error log for any issues during data loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "from cassandra.cluster import Cluster\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from threading import Thread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for database, paths, and retries\n",
    "KEYSPACE = 'financial_data'\n",
    "TABLE = 'test_data_bars_1m_adjsplit'\n",
    "CSV_PATH = '/home/jj/anaconda3/envs/stocks/Database/1M/data'\n",
    "CSV_PATH_TEMP = '/home/jj/anaconda3/envs/stocks//Database/1M/temp'\n",
    "CSV_PATH_LOG = '/home/jj/anaconda3/envs/stocks//Database/1M/log'\n",
    "MAX_RETRIES = 5\n",
    "RETRY_PAUSE = 60  # Duration in seconds to pause between retries\n",
    "SCYLLA_NODE_IP = '192.168.3.41' # Node IP address\n",
    "SCYLLA_NODE_PORT = '9042' # Node port\n",
    "\n",
    "def divide_list_into_chunks(lst, m):\n",
    "    # Split a list into approximately equal-sized chunks\n",
    "    n = len(lst)\n",
    "    chunk_size = n // m\n",
    "    for i in range(0, m - 1):\n",
    "        yield lst[i * chunk_size : (i + 1) * chunk_size]\n",
    "    yield lst[(m - 1) * chunk_size:]\n",
    "\n",
    "def list_files(path):\n",
    "    # Lists all '.txt' files in the provided directory path\n",
    "    all_files = os.listdir(path)\n",
    "    files = list(filter(lambda f: f.endswith('.txt'), all_files))\n",
    "    return files\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005-01-03 09:30:00</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2318</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>173492.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2005-01-03 09:31:00</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2246</td>\n",
       "      <td>17.2246</td>\n",
       "      <td>9646.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005-01-03 09:32:00</td>\n",
       "      <td>17.2246</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2246</td>\n",
       "      <td>17.2318</td>\n",
       "      <td>34810.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005-01-03 09:33:00</td>\n",
       "      <td>17.2318</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2318</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>11464.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2005-01-03 09:34:00</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2318</td>\n",
       "      <td>17.2318</td>\n",
       "      <td>12302.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835693</th>\n",
       "      <td>2023-10-05 15:56:00</td>\n",
       "      <td>110.3200</td>\n",
       "      <td>110.3400</td>\n",
       "      <td>110.2800</td>\n",
       "      <td>110.3200</td>\n",
       "      <td>8780.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835694</th>\n",
       "      <td>2023-10-05 15:57:00</td>\n",
       "      <td>110.3100</td>\n",
       "      <td>110.4350</td>\n",
       "      <td>110.3100</td>\n",
       "      <td>110.3800</td>\n",
       "      <td>13658.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835695</th>\n",
       "      <td>2023-10-05 15:58:00</td>\n",
       "      <td>110.3750</td>\n",
       "      <td>110.4200</td>\n",
       "      <td>110.3650</td>\n",
       "      <td>110.4200</td>\n",
       "      <td>12518.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835696</th>\n",
       "      <td>2023-10-05 15:59:00</td>\n",
       "      <td>110.4200</td>\n",
       "      <td>110.4200</td>\n",
       "      <td>110.3100</td>\n",
       "      <td>110.3200</td>\n",
       "      <td>30497.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835697</th>\n",
       "      <td>2023-10-05 16:01:00</td>\n",
       "      <td>110.3500</td>\n",
       "      <td>110.3500</td>\n",
       "      <td>110.3500</td>\n",
       "      <td>110.3500</td>\n",
       "      <td>195339.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1835698 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0         1         2         3         4         5\n",
       "0        2005-01-03 09:30:00   17.2389   17.2389   17.2318   17.2389  173492.0\n",
       "1        2005-01-03 09:31:00   17.2389   17.2389   17.2246   17.2246    9646.0\n",
       "2        2005-01-03 09:32:00   17.2246   17.2389   17.2246   17.2318   34810.0\n",
       "3        2005-01-03 09:33:00   17.2318   17.2389   17.2318   17.2389   11464.0\n",
       "4        2005-01-03 09:34:00   17.2389   17.2389   17.2318   17.2318   12302.0\n",
       "...                      ...       ...       ...       ...       ...       ...\n",
       "1835693  2023-10-05 15:56:00  110.3200  110.3400  110.2800  110.3200    8780.0\n",
       "1835694  2023-10-05 15:57:00  110.3100  110.4350  110.3100  110.3800   13658.0\n",
       "1835695  2023-10-05 15:58:00  110.3750  110.4200  110.3650  110.4200   12518.0\n",
       "1835696  2023-10-05 15:59:00  110.4200  110.4200  110.3100  110.3200   30497.0\n",
       "1835697  2023-10-05 16:01:00  110.3500  110.3500  110.3500  110.3500  195339.0\n",
       "\n",
       "[1835698 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "symbol = 'A'\n",
    "sample_data = pd.read_csv(f\"{CSV_PATH}/{symbol}_full_1min_adjsplit.txt\", header=None)\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_temp_folder():\n",
    "    \"\"\"Purges the contents of the designated temporary directory.\"\"\"\n",
    "    \n",
    "    for filename in os.listdir(CSV_PATH_TEMP):\n",
    "        file_path = os.path.join(CSV_PATH_TEMP, filename)\n",
    "        \n",
    "        try:\n",
    "            # Identify/remove files or symbolic links\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            # Recognize/delete directories with their contents\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            # Report any deletion issues\n",
    "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "\n",
    "def log_error(symbol, error_message):\n",
    "    \"\"\"\n",
    "    Logs error messages for specific financial symbols to a log file.\n",
    "\n",
    "    Parameters:\n",
    "        symbol (str): The financial instrument identifier.\n",
    "        error_message (str): Description of the issue.\n",
    "    \"\"\"\n",
    "    \n",
    "    log_file = os.path.join(CSV_PATH_LOG, \"error_log.txt\")\n",
    "    \n",
    "    with open(log_file, \"a\") as file:\n",
    "        file.write(f\"Symbol: {symbol} - Error: {error_message}\\n\")\n",
    "\n",
    "def execute_with_retry(command, symbol):\n",
    "    \"\"\"\n",
    "    Executes a command and retries on specific exceptions.\n",
    "    \n",
    "    Parameters:\n",
    "        command (list): The command as a list of strings.\n",
    "        symbol (str): The financial symbol for the command.\n",
    "        \n",
    "    Returns:\n",
    "        subprocess.CompletedProcess: The executed command result.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize retry count\n",
    "    retries = 0\n",
    "    \n",
    "    # Keep trying until max retries\n",
    "    while retries < MAX_RETRIES:\n",
    "        try:\n",
    "            # Run the command, capture output\n",
    "            result = subprocess.run(command, capture_output=True, text=True)\n",
    "            \n",
    "            # Return result if no \"WriteTimeout\" in error\n",
    "            if \"WriteTimeout\" not in result.stderr:\n",
    "                return result\n",
    "            else:\n",
    "                # Else, trigger a retry\n",
    "                raise Exception(\"WriteTimeout encountered\")\n",
    "        except Exception as e:\n",
    "            # Increment retry count if exception\n",
    "            retries += 1\n",
    "            \n",
    "            # Log the error for the symbol\n",
    "            log_error(symbol, str(e))\n",
    "            \n",
    "            # Pause before next retry\n",
    "            time.sleep(RETRY_PAUSE)\n",
    "    \n",
    "    # If here, max retries attempted and failed\n",
    "    print(f\"Max retries reached for {symbol}. Moving on...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_temp_folder():\n",
    "    \"\"\"Purges the contents of the designated temporary directory.\"\"\"\n",
    "    \n",
    "    for filename in os.listdir(CSV_PATH_TEMP):\n",
    "        file_path = os.path.join(CSV_PATH_TEMP, filename)\n",
    "        \n",
    "        try:\n",
    "            # Identify/remove files or symbolic links\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            # Recognize/delete directories with their contents\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            # Report any deletion issues\n",
    "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "\n",
    "def log_error(symbol, error_message):\n",
    "    \"\"\"\n",
    "    Logs error messages for specific financial symbols to a log file.\n",
    "\n",
    "    Parameters:\n",
    "        symbol (str): The financial instrument identifier.\n",
    "        error_message (str): Description of the issue.\n",
    "    \"\"\"\n",
    "    \n",
    "    log_file = os.path.join(CSV_PATH_LOG, \"error_log.txt\")\n",
    "    \n",
    "    with open(log_file, \"a\") as file:\n",
    "        file.write(f\"Symbol: {symbol} - Error: {error_message}\\n\")\n",
    "\n",
    "def execute_with_retry(command, symbol):\n",
    "    \"\"\"\n",
    "    Executes a command and retries on specific exceptions.\n",
    "    \n",
    "    Parameters:\n",
    "        command (list): The command as a list of strings.\n",
    "        symbol (str): The financial symbol for the command.\n",
    "        \n",
    "    Returns:\n",
    "        subprocess.CompletedProcess: The executed command result.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize retry count\n",
    "    retries = 0\n",
    "    \n",
    "    # Keep trying until max retries\n",
    "    while retries < MAX_RETRIES:\n",
    "        try:\n",
    "            # Run the command, capture output\n",
    "            result = subprocess.run(command, capture_output=True, text=True)\n",
    "            \n",
    "            # Return result if no \"WriteTimeout\" in error\n",
    "            if \"WriteTimeout\" not in result.stderr:\n",
    "                return result\n",
    "            else:\n",
    "                # Else, trigger a retry\n",
    "                raise Exception(\"WriteTimeout encountered\")\n",
    "        except Exception as e:\n",
    "            # Increment retry count if exception\n",
    "            retries += 1\n",
    "            \n",
    "            # Log the error for the symbol\n",
    "            log_error(symbol, str(e))\n",
    "            \n",
    "            # Pause before next retry\n",
    "            time.sleep(RETRY_PAUSE)\n",
    "    \n",
    "    # If here, max retries attempted and failed\n",
    "    print(f\"Max retries reached for {symbol}. Moving on...\")\n",
    "\n",
    "def create_keyspace_and_table():\n",
    "    \"\"\"\n",
    "    Establishes a connection to Scylla DB and initializes a keyspace and a \n",
    "    table if they aren't present. This function serves as the initial setup \n",
    "    phase, ensuring the database is ready for incoming data. The table stores \n",
    "    financial data like stock prices optimizing query performance.\n",
    "      \n",
    "    Steps:\n",
    "    1. Connect to the Scylla cluster.\n",
    "    2. Create keyspace if absent. It uses a simple replication strategy with \n",
    "       a factor of 1, implying data on one node. This may not be optimal for \n",
    "       production systems where redundancy is vital.\n",
    "    3. Set active keyspace for the session.\n",
    "    4. Design and initialize the table. Schema has:\n",
    "       - Composite primary key for efficient time-based queries.\n",
    "       - 'TimeWindowCompactionStrategy' for time series data in 30-day windows.\n",
    "    5. Terminate the session and connection after operations.\n",
    "    \"\"\"\n",
    "    \n",
    "    cluster = Cluster([SCYLLA_NODE_IP], port=SCYLLA_NODE_PORT)\n",
    "    session = cluster.connect()\n",
    "    \n",
    "    session.execute(f\"CREATE KEYSPACE IF NOT EXISTS {KEYSPACE} \"\n",
    "                    f\"WITH replication = {{'class': 'SimpleStrategy', \"\n",
    "                    f\"'replication_factor' : 1}};\")\n",
    "    session.set_keyspace(KEYSPACE)\n",
    "    \n",
    "    session.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {KEYSPACE}.{TABLE} (\n",
    "        symbol text,\n",
    "        bucket text,\n",
    "        timestamp text,\n",
    "        open float,\n",
    "        high float,\n",
    "        low float,\n",
    "        close float,\n",
    "        volume float,\n",
    "        PRIMARY KEY ((symbol, bucket), timestamp)\n",
    "    ) WITH compaction = {{\n",
    "        'class': 'TimeWindowCompactionStrategy',\n",
    "        'compaction_window_unit': 'DAYS',\n",
    "        'compaction_window_size': 30\n",
    "    }};\n",
    "    \"\"\")\n",
    "    \n",
    "    # The decision to not use the default_time_to_live property in the table \n",
    "    # definition was made to ensure continuous access to historical financial data.\n",
    "    \n",
    "    session.shutdown()\n",
    "    cluster.shutdown()\n",
    "\n",
    "def get_bucket_from_timestamp(timestamp):\n",
    "    dt_obj = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n",
    "    return f\"{dt_obj.year}-{dt_obj.month:02}\"\n",
    "\n",
    "def preprocess_and_load(symbol):\n",
    "    \"\"\"\n",
    "    Preprocesses and loads data for a symbol into the database.\n",
    "    \n",
    "    Parameters:\n",
    "        symbol (str): The symbol to be processed.\n",
    "        \n",
    "    Returns:\n",
    "        int: The number of entries processed for the symbol.\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(CSV_PATH, f\"{symbol}_full_1min_adjsplit.txt\")\n",
    "    temp_file = os.path.join(CSV_PATH_TEMP, f\"{symbol}_temp.txt\")\n",
    "    \n",
    "    line_count = 0\n",
    "    \n",
    "    with open(input_file, 'r') as source, open(temp_file, 'w') as target:\n",
    "        for line in source:\n",
    "            line_count += 1\n",
    "            data = line.strip().split(',')\n",
    "            timestamp = data[0]\n",
    "            bucket = get_bucket_from_timestamp(timestamp)\n",
    "            target.write(f\"{symbol},{bucket},{timestamp},{','.join(data[1:])}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    copy_cmd = ['/home/jj/anaconda3/envs/stocks/bin/cqlsh', '-e',\n",
    "                f\"COPY {KEYSPACE}.{TABLE} FROM '{temp_file}' \"\n",
    "                f\"WITH DELIMITER=',' AND HEADER=FALSE\"]\n",
    "    \n",
    "    result = execute_with_retry(copy_cmd, symbol)\n",
    "    os.remove(temp_file)\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "    \n",
    "    return line_count, time_taken\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    \"\"\"\n",
    "    Process a subset (chunk) of financial symbol data files.\n",
    "    Preprocesses and loads data for each symbol in the chunk.\n",
    "    \"\"\"\n",
    "    total_rows = 0\n",
    "    total_time = 0\n",
    "    chunk_start_time = time.time()  # Mark the start time for the chunk processing\n",
    "\n",
    "    for symbol in chunk:\n",
    "        rows, time_taken = preprocess_and_load(symbol)\n",
    "        total_rows += rows\n",
    "        total_time += time_taken\n",
    "        progress_queue.put(1)\n",
    "\n",
    "    chunk_end_time = time.time()  # Mark the end time for the chunk processing\n",
    "    chunk_total_time = chunk_end_time - chunk_start_time  # Calculate total time taken for the chunk\n",
    "\n",
    "    return total_rows, total_time, round(chunk_total_time, 2)  # Return the total time taken for the chunk\n",
    "\n",
    "\n",
    "def monitor_progress():\n",
    "    \"\"\"\n",
    "    Monitors the progress of the data loading process.\n",
    "    Counts processed symbols and updates the progress bar.\n",
    "    \"\"\"\n",
    "    processed = 0\n",
    "    while processed < len(files_list):\n",
    "        _ = progress_queue.get()\n",
    "        pbar.update(1)\n",
    "        processed += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9900f467cf841c38caa5246034a1c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total rows inserted across processes: 57,091,172 rows\n",
      "\n",
      "Total time for all processes: 229.69 seconds\n",
      "Average insertion rate for all processes combined: 248,552 rows/sec\n",
      "Average individual insertion rate: 80,665 rows/sec\n",
      "\n",
      "Individual process times: [229.69, 166.77, 172.61, 148.56, 206.05]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Main script execution starts.\n",
    "    \n",
    "    # Clear any existing temporary files.\n",
    "    clear_temp_folder()\n",
    "    # Create the required keyspace and table in ScyllaDB.\n",
    "    create_keyspace_and_table()\n",
    "    # List all the financial data files.\n",
    "    files = list_files(CSV_PATH)\n",
    "    # Extract symbol names from file names.\n",
    "    files_list = [element.replace('_full_1min_adjsplit.txt', '') for element in files]\n",
    "    # Sort the file list for consistent processing order.\n",
    "    files_list.sort()\n",
    "    # Initialize a multiprocessing queue to track progress.\n",
    "    progress_queue = mp.Queue()\n",
    "    processes = 5\n",
    "    # (Optional) Limit to the first 10 files for processing.\n",
    "    files_list = files_list[0:100]\n",
    "    # Divide the file list into chunks for parallel processing.\n",
    "    chunks = divide_list_into_chunks(files_list, processes)\n",
    "    # Initialize a progress bar using tqdm.\n",
    "    pbar = tqdm(total=len(files_list))\n",
    "    # Start a separate thread to monitor processing progress.\n",
    "    t = Thread(target=monitor_progress)\n",
    "    t.start()\n",
    "    with mp.Pool(processes=processes) as pool:\n",
    "        global_start_time = time.time()\n",
    "        results = []\n",
    "        for chunk in chunks:\n",
    "            results.append(pool.apply_async(process_chunk, (chunk,)))\n",
    "        results = [res.get() for res in results]\n",
    "        global_end_time = time.time()\n",
    "        \n",
    "        # Aggregate the number of rows processed and time taken across all chunks\n",
    "        total_rows_aggregated = sum([r[0] for r in results])\n",
    "        total_time_aggregated = sum([r[1] for r in results])\n",
    "        process_times = [r[2] for r in results]  # List of total execution times for each process\n",
    "\n",
    "        print(f\"\\nTotal rows inserted across processes: {total_rows_aggregated:,.0f} rows\")\n",
    "        print(f\"\\nTotal time for all processes: {global_end_time - global_start_time:.2f} seconds\")\n",
    "        print(f\"Average insertion rate for all processes combined: {total_rows_aggregated / (global_end_time - global_start_time):,.0f} rows/sec\")\n",
    "        print(f\"Average individual insertion rate: {total_rows_aggregated / total_time_aggregated:,.0f} rows/sec\")\n",
    "        print(f\"\\nIndividual process times: {process_times}\")\n",
    "\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Fetcher from the Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fetch_minute_data` Function Overview:\n",
    "\n",
    "The `fetch_minute_data` function is designed to pull minute-level financial data from a ScyllaDB (or Cassandra) database. With the assistance of pandas, it provides several functionalities:\n",
    "\n",
    "- **Custom Date Ranges:** Users can set a specific start and end times for fetching the dataset.\n",
    "- **Trading Hours Filter:** Provides an option to retrieve data only from standard trading hours.\n",
    "- **Month-based Bucketing System:** Organizes data for efficient retrieval.\n",
    "- **Asynchronous Data Fetching:** Enables parallel data retrieval across different time intervals.\n",
    "- **Data Rounding:** Users have the option to round numerical values in the dataset to a precision of four decimal places.\n",
    "\n",
    "For a smooth operation, ensure there's an operational ScyllaDB or Cassandra cluster, an appropriate Python environment, and the necessary keyspace and table references.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from cassandra.cluster import Cluster\n",
    "\n",
    "def fetch_minute_data(symbol, start_time, end_time, KEYSPACE, TABLE, trading_hours_only, rounding):\n",
    "    \"\"\"\n",
    "    Fetch minute-level financial data for a symbol and timeframe from ScyllaDB/Cassandra.\n",
    "    \n",
    "    Parameters:\n",
    "    - symbol (str): Ticker symbol for data retrieval.\n",
    "    - start_time (str): Start of desired timeframe 'YYYY-MM-DD HH:MM:SS'.\n",
    "    - end_time (str): End of desired timeframe 'YYYY-MM-DD HH:MM:SS'.\n",
    "    - KEYSPACE (str): ScyllaDB/Cassandra keyspace to connect.\n",
    "    - TABLE (str): Table within the keyspace to query.\n",
    "    \n",
    "    Returns:\n",
    "    - dataframe (pd.DataFrame): pandas DataFrame with queried financial data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Helper to generate monthly bucket strings for data partitioning\n",
    "    def generate_monthly_buckets(start_time, end_time):\n",
    "        start_date = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n",
    "        end_date = datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S')\n",
    "        current_date = start_date\n",
    "        buckets = []\n",
    "        \n",
    "        # Loop through each month to generate bucket strings\n",
    "        while current_date <= end_date:\n",
    "            buckets.append(f\"{current_date.year}-{current_date.month:02}\")\n",
    "            \n",
    "            # Move to next month\n",
    "            current_date = current_date + timedelta(days=31)\n",
    "            current_date = datetime(current_date.year, current_date.month, 1)\n",
    "        return buckets\n",
    "\n",
    "    # Generate necessary bucket strings for timeframe\n",
    "    buckets = generate_monthly_buckets(start_time, end_time)\n",
    "    \n",
    "    # List to store dataframes for each bucket\n",
    "    dfs = []\n",
    "    \n",
    "    # Connect to ScyllaDB/Cassandra cluster and keyspace\n",
    "    cluster = Cluster([SCYLLA_NODE_IP], port=SCYLLA_NODE_PORT)\n",
    "    session = cluster.connect(KEYSPACE)\n",
    "    \n",
    "    # Prepare data fetch query\n",
    "    query = f\"\"\"SELECT timestamp, open, high, low, close, volume \n",
    "                FROM {TABLE} WHERE symbol = ? AND bucket = ? AND \n",
    "                \\\"timestamp\\\" >= ? AND \\\"timestamp\\\" <= ?\n",
    "            \"\"\"\n",
    "    prepared = session.prepare(query)\n",
    "    futures = []\n",
    "\n",
    "    # Asynchronously fetch data for each bucket\n",
    "    for bucket in buckets:\n",
    "        future = session.execute_async(prepared, (symbol, bucket, start_time, end_time))\n",
    "        futures.append(future)\n",
    "\n",
    "    # Convert query results to pandas dataframes\n",
    "    for future in futures:\n",
    "        rows = future.result()\n",
    "        dfs.append(pd.DataFrame(list(rows)))\n",
    "\n",
    "    # Close session and cluster connection after fetching\n",
    "    session.shutdown()\n",
    "    cluster.shutdown()\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    dataframe = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Set timestamp as index and remove timestamp column\n",
    "    dataframe.index = dataframe['timestamp'].astype('datetime64[ns]') # type: ignore\n",
    "    dataframe.drop('timestamp', inplace=True, axis=1)\n",
    "\n",
    "    if trading_hours_only:\n",
    "        dataframe = dataframe.between_time('09:30:00', '16:00:00')\n",
    "    if rounding:\n",
    "        dataframe = dataframe.round(4)\n",
    "\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-01-03 09:30:00</th>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2318</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>173492.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-03 09:31:00</th>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2246</td>\n",
       "      <td>17.2246</td>\n",
       "      <td>9646.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-03 09:32:00</th>\n",
       "      <td>17.2246</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2246</td>\n",
       "      <td>17.2318</td>\n",
       "      <td>34810.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-03 09:33:00</th>\n",
       "      <td>17.2318</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2318</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>11464.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-03 09:34:00</th>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2318</td>\n",
       "      <td>17.2318</td>\n",
       "      <td>12302.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-05 15:56:00</th>\n",
       "      <td>110.3200</td>\n",
       "      <td>110.3400</td>\n",
       "      <td>110.2800</td>\n",
       "      <td>110.3200</td>\n",
       "      <td>8780.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-05 15:57:00</th>\n",
       "      <td>110.3100</td>\n",
       "      <td>110.4350</td>\n",
       "      <td>110.3100</td>\n",
       "      <td>110.3800</td>\n",
       "      <td>13658.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-05 15:58:00</th>\n",
       "      <td>110.3750</td>\n",
       "      <td>110.4200</td>\n",
       "      <td>110.3650</td>\n",
       "      <td>110.4200</td>\n",
       "      <td>12518.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-05 15:59:00</th>\n",
       "      <td>110.4200</td>\n",
       "      <td>110.4200</td>\n",
       "      <td>110.3100</td>\n",
       "      <td>110.3200</td>\n",
       "      <td>30497.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-05 16:01:00</th>\n",
       "      <td>110.3500</td>\n",
       "      <td>110.3500</td>\n",
       "      <td>110.3500</td>\n",
       "      <td>110.3500</td>\n",
       "      <td>195339.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1835698 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open      high       low     close    volume\n",
       "timestamp                                                            \n",
       "2005-01-03 09:30:00   17.2389   17.2389   17.2318   17.2389  173492.0\n",
       "2005-01-03 09:31:00   17.2389   17.2389   17.2246   17.2246    9646.0\n",
       "2005-01-03 09:32:00   17.2246   17.2389   17.2246   17.2318   34810.0\n",
       "2005-01-03 09:33:00   17.2318   17.2389   17.2318   17.2389   11464.0\n",
       "2005-01-03 09:34:00   17.2389   17.2389   17.2318   17.2318   12302.0\n",
       "...                       ...       ...       ...       ...       ...\n",
       "2023-10-05 15:56:00  110.3200  110.3400  110.2800  110.3200    8780.0\n",
       "2023-10-05 15:57:00  110.3100  110.4350  110.3100  110.3800   13658.0\n",
       "2023-10-05 15:58:00  110.3750  110.4200  110.3650  110.4200   12518.0\n",
       "2023-10-05 15:59:00  110.4200  110.4200  110.3100  110.3200   30497.0\n",
       "2023-10-05 16:01:00  110.3500  110.3500  110.3500  110.3500  195339.0\n",
       "\n",
       "[1835698 rows x 5 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol = 'A'\n",
    "KEYSPACE = 'financial_data'\n",
    "TABLE = 'test_data_bars_1m_adjsplit'\n",
    "ohlc_df = fetch_minute_data(\n",
    "    symbol, \n",
    "    '2005-01-01 00:00:00', \n",
    "    '2023-10-05 23:59:00',                      \n",
    "    KEYSPACE, \n",
    "    TABLE, \n",
    "    trading_hours_only=False, \n",
    "    rounding=True\n",
    ")\n",
    "ohlc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005-01-03 09:30:00</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2318</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>173492.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2005-01-03 09:31:00</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2246</td>\n",
       "      <td>17.2246</td>\n",
       "      <td>9646.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005-01-03 09:32:00</td>\n",
       "      <td>17.2246</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2246</td>\n",
       "      <td>17.2318</td>\n",
       "      <td>34810.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005-01-03 09:33:00</td>\n",
       "      <td>17.2318</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2318</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>11464.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2005-01-03 09:34:00</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2318</td>\n",
       "      <td>17.2318</td>\n",
       "      <td>12302.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835693</th>\n",
       "      <td>2023-10-05 15:56:00</td>\n",
       "      <td>110.3200</td>\n",
       "      <td>110.3400</td>\n",
       "      <td>110.2800</td>\n",
       "      <td>110.3200</td>\n",
       "      <td>8780.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835694</th>\n",
       "      <td>2023-10-05 15:57:00</td>\n",
       "      <td>110.3100</td>\n",
       "      <td>110.4350</td>\n",
       "      <td>110.3100</td>\n",
       "      <td>110.3800</td>\n",
       "      <td>13658.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835695</th>\n",
       "      <td>2023-10-05 15:58:00</td>\n",
       "      <td>110.3750</td>\n",
       "      <td>110.4200</td>\n",
       "      <td>110.3650</td>\n",
       "      <td>110.4200</td>\n",
       "      <td>12518.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835696</th>\n",
       "      <td>2023-10-05 15:59:00</td>\n",
       "      <td>110.4200</td>\n",
       "      <td>110.4200</td>\n",
       "      <td>110.3100</td>\n",
       "      <td>110.3200</td>\n",
       "      <td>30497.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835697</th>\n",
       "      <td>2023-10-05 16:01:00</td>\n",
       "      <td>110.3500</td>\n",
       "      <td>110.3500</td>\n",
       "      <td>110.3500</td>\n",
       "      <td>110.3500</td>\n",
       "      <td>195339.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1835698 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0         1         2         3         4         5\n",
       "0        2005-01-03 09:30:00   17.2389   17.2389   17.2318   17.2389  173492.0\n",
       "1        2005-01-03 09:31:00   17.2389   17.2389   17.2246   17.2246    9646.0\n",
       "2        2005-01-03 09:32:00   17.2246   17.2389   17.2246   17.2318   34810.0\n",
       "3        2005-01-03 09:33:00   17.2318   17.2389   17.2318   17.2389   11464.0\n",
       "4        2005-01-03 09:34:00   17.2389   17.2389   17.2318   17.2318   12302.0\n",
       "...                      ...       ...       ...       ...       ...       ...\n",
       "1835693  2023-10-05 15:56:00  110.3200  110.3400  110.2800  110.3200    8780.0\n",
       "1835694  2023-10-05 15:57:00  110.3100  110.4350  110.3100  110.3800   13658.0\n",
       "1835695  2023-10-05 15:58:00  110.3750  110.4200  110.3650  110.4200   12518.0\n",
       "1835696  2023-10-05 15:59:00  110.4200  110.4200  110.3100  110.3200   30497.0\n",
       "1835697  2023-10-05 16:01:00  110.3500  110.3500  110.3500  110.3500  195339.0\n",
       "\n",
       "[1835698 rows x 6 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data # To visually compare with original data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
