{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview:\n",
    "\n",
    "This utility script provides a streamlined way to process and load large volumes of financial data into a ScyllaDB database. It primarily targets minute-resolution, adjusted-split financial bars but can be adapted for different data structures. In addition to its core functionalities, it offers the `fetch_minute_data` function to retrieve specific minute-level datasets from the database. The script uses efficient techniques such as multiprocessing, batching, and asynchronous fetching to enhance speed and consistency.\n",
    "\n",
    "---\n",
    "\n",
    "## Features:\n",
    "\n",
    "- **Parallel Processing:** Utilizes Python's multiprocessing library to parallelize data processing, making full use of available CPU cores.\n",
    "- **Automatic Retries:** Implements a retry mechanism for data insertions, ensuring data consistency even in the face of transient database errors.\n",
    "- **Progress Monitoring:** Offers a real-time progress bar to keep track of the data loading process.\n",
    "- **Error Logging:** Captures and logs errors encountered during data processing and loading for easier troubleshooting.\n",
    "- **Data Preprocessing:** Preprocesses raw data files to extract relevant information and organize it into a more database-friendly format.\n",
    "- **Custom Date Ranges:** Users can set specific start and end times using `fetch_minute_data`.\n",
    "- **Trading Hours Filter:** Provides an option to retrieve data only from standard trading hours.\n",
    "- **Month-based Bucketing System:** Organizes data for efficient retrieval.\n",
    "- **Asynchronous Data Fetching:** Enables parallel data retrieval across different time intervals.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup and Usage:\n",
    "\n",
    "### Prerequisites:\n",
    "- Python 3.11.4\n",
    "- ScyllaDB\n",
    "- Python packages: `os`, `shutil`, `subprocess`, `multiprocessing`, `cassandra-driver`, `tqdm`, `datetime`\n",
    "\n",
    "### Configuration:\n",
    "1. Adjust the constants at the beginning of the script, such as `KEYSPACE`, `TABLE`, and `CSV_PATH`, to match your setup.\n",
    "2. Ensure the ScyllaDB cluster is up and accessible from the script execution environment.\n",
    "\n",
    "### Execution:\n",
    "- Simply run the cells sequentially. By default, the script processes a specific number of .txt files located in the directory specified by `CSV_PATH`. Adjust as needed for processing different file counts.\n",
    "\n",
    "---\n",
    "\n",
    "## Function Descriptions:\n",
    "\n",
    "- `divide_list_into_chunks()`: Splits a list into approximately equal-sized chunks for parallel processing.\n",
    "- `list_files()`: Lists all '.txt' files in the specified directory.\n",
    "- `create_keyspace_and_table()`: Establishes a connection to ScyllaDB and creates a keyspace and table if not already present.\n",
    "- `clear_temp_folder()`: Clears any existing temporary files before processing starts.\n",
    "- `log_error()`: Logs errors encountered during data processing.\n",
    "- `execute_with_retry()`: Retries data insertion in case of transient database errors.\n",
    "- `get_bucket_from_timestamp()`: Extracts year-month info from timestamps, useful for bucketing data.\n",
    "- `preprocess_and_load()`: Processes each financial data file and loads the data into ScyllaDB.\n",
    "- `process_chunk()`: Handles a chunk of data files, invoking preprocessing and loading for each.\n",
    "- `monitor_progress()`: A thread-safe function that updates the progress bar as data files are processed.\n",
    "- `fetch_minute_data`: Retrieves minute-level financial data from the database based on specified criteria.\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** Always ensure you have backups of your data and that you've set up appropriate monitoring and alerting for your ScyllaDB cluster. Regularly check the error log for any issues during data loading.\n",
    "\n",
    "The notebook was created and tested in Visual Studio Code. It is not guaranteed that it will work in a different environment.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "from cassandra.cluster import Cluster\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from threading import Thread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for database, paths, and retries\n",
    "KEYSPACE = 'financial_data'\n",
    "TABLE = 'test_data_bars_1m_adjsplit'\n",
    "CSV_PATH = '/home/jj/anaconda3/envs/stocks/Database/1M/data'\n",
    "CSV_PATH_TEMP = '/home/jj/anaconda3/envs/stocks/Database/1M/temp'\n",
    "CSV_PATH_LOG = '/home/jj/anaconda3/envs/stocks/Database/1M/log'\n",
    "MAX_RETRIES = 5\n",
    "RETRY_PAUSE = 60  # Duration in seconds to pause between retries\n",
    "SCYLLA_NODE_IP = '192.168.3.41' # Node IP address\n",
    "SCYLLA_NODE_PORT = '9042' # Node port\n",
    "\n",
    "def divide_list_into_chunks(lst, m):\n",
    "    # Split a list into approximately equal-sized chunks\n",
    "    n = len(lst)\n",
    "    chunk_size = n // m\n",
    "    for i in range(0, m - 1):\n",
    "        yield lst[i * chunk_size : (i + 1) * chunk_size]\n",
    "    yield lst[(m - 1) * chunk_size:]\n",
    "\n",
    "def list_files(path):\n",
    "    # Lists all '.txt' files in the provided directory path\n",
    "    all_files = os.listdir(path)\n",
    "    files = list(filter(lambda f: f.endswith('.txt'), all_files))\n",
    "    return files\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005-01-03 09:30:00</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2318</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>173492.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2005-01-03 09:31:00</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2246</td>\n",
       "      <td>17.2246</td>\n",
       "      <td>9646.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005-01-03 09:32:00</td>\n",
       "      <td>17.2246</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2246</td>\n",
       "      <td>17.2318</td>\n",
       "      <td>34810.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005-01-03 09:33:00</td>\n",
       "      <td>17.2318</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2318</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>11464.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2005-01-03 09:34:00</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2389</td>\n",
       "      <td>17.2318</td>\n",
       "      <td>17.2318</td>\n",
       "      <td>12302.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835693</th>\n",
       "      <td>2023-10-05 15:56:00</td>\n",
       "      <td>110.3200</td>\n",
       "      <td>110.3400</td>\n",
       "      <td>110.2800</td>\n",
       "      <td>110.3200</td>\n",
       "      <td>8780.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835694</th>\n",
       "      <td>2023-10-05 15:57:00</td>\n",
       "      <td>110.3100</td>\n",
       "      <td>110.4350</td>\n",
       "      <td>110.3100</td>\n",
       "      <td>110.3800</td>\n",
       "      <td>13658.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835695</th>\n",
       "      <td>2023-10-05 15:58:00</td>\n",
       "      <td>110.3750</td>\n",
       "      <td>110.4200</td>\n",
       "      <td>110.3650</td>\n",
       "      <td>110.4200</td>\n",
       "      <td>12518.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835696</th>\n",
       "      <td>2023-10-05 15:59:00</td>\n",
       "      <td>110.4200</td>\n",
       "      <td>110.4200</td>\n",
       "      <td>110.3100</td>\n",
       "      <td>110.3200</td>\n",
       "      <td>30497.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835697</th>\n",
       "      <td>2023-10-05 16:01:00</td>\n",
       "      <td>110.3500</td>\n",
       "      <td>110.3500</td>\n",
       "      <td>110.3500</td>\n",
       "      <td>110.3500</td>\n",
       "      <td>195339.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1835698 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0         1         2         3         4         5\n",
       "0        2005-01-03 09:30:00   17.2389   17.2389   17.2318   17.2389  173492.0\n",
       "1        2005-01-03 09:31:00   17.2389   17.2389   17.2246   17.2246    9646.0\n",
       "2        2005-01-03 09:32:00   17.2246   17.2389   17.2246   17.2318   34810.0\n",
       "3        2005-01-03 09:33:00   17.2318   17.2389   17.2318   17.2389   11464.0\n",
       "4        2005-01-03 09:34:00   17.2389   17.2389   17.2318   17.2318   12302.0\n",
       "...                      ...       ...       ...       ...       ...       ...\n",
       "1835693  2023-10-05 15:56:00  110.3200  110.3400  110.2800  110.3200    8780.0\n",
       "1835694  2023-10-05 15:57:00  110.3100  110.4350  110.3100  110.3800   13658.0\n",
       "1835695  2023-10-05 15:58:00  110.3750  110.4200  110.3650  110.4200   12518.0\n",
       "1835696  2023-10-05 15:59:00  110.4200  110.4200  110.3100  110.3200   30497.0\n",
       "1835697  2023-10-05 16:01:00  110.3500  110.3500  110.3500  110.3500  195339.0\n",
       "\n",
       "[1835698 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "symbol = 'A'\n",
    "sample_data = pd.read_csv(f\"{CSV_PATH}/{symbol}_full_1min_adjsplit.txt\", header=None)\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_temp_folder():\n",
    "    \"\"\"Purges the contents of the designated temporary directory.\"\"\"\n",
    "    \n",
    "    for filename in os.listdir(CSV_PATH_TEMP):\n",
    "        file_path = os.path.join(CSV_PATH_TEMP, filename)\n",
    "        \n",
    "        try:\n",
    "            # Identify and remove files or symbolic links\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            # Recognize and delete directories along with their inner contents\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            # Capture and report any encountered deletion issues\n",
    "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "\n",
    "def log_error(symbol, error_message):\n",
    "    \"\"\"\n",
    "    Appends error messages related to specific financial symbols to a designated log file.\n",
    "\n",
    "    Parameters:\n",
    "        symbol (str): The identifier of the financial instrument or stock.\n",
    "        error_message (str): A concise description of the encountered issue.\n",
    "    \"\"\"\n",
    "    \n",
    "    log_file = os.path.join(CSV_PATH_LOG, \"error_log.txt\")\n",
    "    \n",
    "    with open(log_file, \"a\") as file:\n",
    "        file.write(f\"Symbol: {symbol} - Error: {error_message}\\n\")\n",
    "\n",
    "def execute_with_retry(command, symbol):\n",
    "    \"\"\"\n",
    "    Tries executing a given command and retries in case of specific exceptions.\n",
    "    \n",
    "    Parameters:\n",
    "        command (list): The command to be executed as a list of strings.\n",
    "        symbol (str): The financial symbol for which the command is being executed.\n",
    "        \n",
    "    Returns:\n",
    "        subprocess.CompletedProcess: The result of the executed command.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the retry count\n",
    "    retries = 0\n",
    "    \n",
    "    # Keep trying until reaching the maximum allowed retries\n",
    "    while retries < MAX_RETRIES:\n",
    "        try:\n",
    "            # Run the command and capture its output\n",
    "            result = subprocess.run(command, capture_output=True, text=True)\n",
    "            \n",
    "            # If the error stream does not indicate a \"WriteTimeout\", return the result\n",
    "            if \"WriteTimeout\" not in result.stderr:\n",
    "                return result\n",
    "            else:\n",
    "                # Otherwise, raise a custom exception to trigger a retry\n",
    "                raise Exception(\"WriteTimeout encountered\")\n",
    "        except Exception as e:\n",
    "            # If any exception occurs, increment the retry count\n",
    "            retries += 1\n",
    "            \n",
    "            # Log the error for the specific symbol and the exception encountered\n",
    "            log_error(symbol, str(e))\n",
    "            \n",
    "            # Pause the execution for a specified duration before the next retry\n",
    "            time.sleep(RETRY_PAUSE)\n",
    "    \n",
    "    # If the function reaches here, it means max retries were attempted and all failed\n",
    "    print(f\"Max retries reached for symbol {symbol}. Moving on to the next file...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keyspace_and_table():\n",
    "    \"\"\"\n",
    "    Establishes a connection to Scylla DB, then initializes a keyspace and a corresponding table if they aren't present.\n",
    "\n",
    "    This function serves as the initial setup phase in the data processing pipeline, ensuring the database is ready\n",
    "    for incoming data. The table is designed to store financial data like stock prices, and its schema optimizes query \n",
    "    performance for time-based lookups and data compactness.\n",
    "\n",
    "    Steps:\n",
    "    1. Establish a connection to the Scylla cluster.\n",
    "    2. Create the desired keyspace, if it's absent. The keyspace is equipped with a simple replication strategy with \n",
    "       a factor of 1, which implies data is only stored on one node. This strategy might not be optimal for \n",
    "       production systems, where redundancy is crucial.\n",
    "    3. Set the active keyspace for the session.\n",
    "    4. Design and initialize the table. The schema is crafted with:\n",
    "       - A composite primary key consisting of the symbol, bucket (year-month), and timestamp. This design allows \n",
    "         efficient time-based queries within a specific stock symbol and date range.\n",
    "       - The 'TimeWindowCompactionStrategy' is employed, which is optimal for time series data. It groups data by \n",
    "         windows of time, compacting and purging obsolete data. The strategy used here compacts data in 30-day windows.\n",
    "    5. Once the database operations are complete, the session and the connection to the cluster are terminated.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Connect to the Scylla cluster\n",
    "    cluster = Cluster([SCYLLA_NODE_IP], port=SCYLLA_NODE_PORT)\n",
    "    session = cluster.connect()\n",
    "    \n",
    "    # Create the keyspace if it doesn't exist\n",
    "    session.execute(f\"CREATE KEYSPACE IF NOT EXISTS {KEYSPACE} WITH replication = {{'class': 'SimpleStrategy', 'replication_factor' : 1}};\")\n",
    "    session.set_keyspace(KEYSPACE)\n",
    "    \n",
    "    # Define and create the table if it's not already there\n",
    "    session.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {KEYSPACE}.{TABLE} (\n",
    "        symbol text,\n",
    "        bucket text,\n",
    "        timestamp text,\n",
    "        open float,\n",
    "        high float,\n",
    "        low float,\n",
    "        close float,\n",
    "        volume float,\n",
    "        PRIMARY KEY ((symbol, bucket), timestamp)\n",
    "    ) WITH compaction = {{\n",
    "        'class': 'TimeWindowCompactionStrategy',\n",
    "        'compaction_window_unit': 'DAYS',\n",
    "        'compaction_window_size': 30\n",
    "    }};\n",
    "    \"\"\")\n",
    "    \n",
    "    # Disconnect from the cluster and end the session\n",
    "    session.shutdown()\n",
    "    cluster.shutdown()\n",
    "\n",
    "def get_bucket_from_timestamp(timestamp):\n",
    "    # Extracts year-month information from a timestamp\n",
    "    dt_obj = datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n",
    "    return f\"{dt_obj.year}-{dt_obj.month:02}\"\n",
    "\n",
    "def preprocess_and_load(symbol):\n",
    "    \"\"\"\n",
    "    Preprocesses and then loads data for a specific financial symbol into the database.\n",
    "    \n",
    "    Parameters:\n",
    "        symbol (str): The financial symbol to be processed.\n",
    "        \n",
    "    Returns:\n",
    "        int: The number of lines (data entries) processed for the given symbol.\n",
    "    \"\"\"\n",
    "    # Notify the user about the symbol currently being processed\n",
    "    # print(f\"Processing data for symbol: {symbol}\")\n",
    "\n",
    "    # Construct the paths for the input and temporary files\n",
    "    input_file = os.path.join(CSV_PATH, f\"{symbol}_full_1min_adjsplit.txt\")\n",
    "    temp_file = os.path.join(CSV_PATH_TEMP, f\"{symbol}_temp.txt\")\n",
    "    \n",
    "    line_count = 0  # Counter to track the number of lines processed\n",
    "    \n",
    "    # Open both the source file (input_file) and a temporary file (temp_file)\n",
    "    with open(input_file, 'r') as source, open(temp_file, 'w') as target:\n",
    "        # Loop through each line in the source file\n",
    "        for line in source:\n",
    "            line_count += 1  # Increment the line counter\n",
    "            \n",
    "            # Split the data and extract the timestamp\n",
    "            data = line.strip().split(',')\n",
    "            timestamp = data[0]\n",
    "            \n",
    "            # Get the year-month bucket based on the timestamp\n",
    "            bucket = get_bucket_from_timestamp(timestamp)\n",
    "            \n",
    "            # Write the preprocessed data into the temporary file\n",
    "            target.write(f\"{symbol},{bucket},{timestamp},{','.join(data[1:])}\\n\")\n",
    "    \n",
    "    # Construct the command to copy data from the temporary file to ScyllaDB\n",
    "    copy_cmd = ['./bin/cqlsh', '-e', f\"COPY {KEYSPACE}.{TABLE} (symbol, bucket, timestamp, open, high, low, close, volume) FROM '{temp_file}' WITH DELIMITER=',' AND HEADER=FALSE\"]\n",
    "    \n",
    "    # Execute the copy command with retries in case of WriteTimeout errors\n",
    "    result = execute_with_retry(copy_cmd, symbol)\n",
    "    \n",
    "    # If there is any output or errors from the command, print them\n",
    "    # if result:\n",
    "    #     print(result.stdout)\n",
    "    #     print(result.stderr)\n",
    "    \n",
    "    # Remove the temporary file after successfully copying data to the database\n",
    "    os.remove(temp_file)\n",
    "    \n",
    "    return line_count\n",
    "\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    \"\"\"\n",
    "    Process a subset (chunk) of financial symbol data files.\n",
    "    The function will preprocess and load data for each symbol in the chunk.\n",
    "    \"\"\"\n",
    "    total_rows = 0\n",
    "    for symbol in chunk:\n",
    "        # Preprocess and load data for the current symbol and count the number of rows.\n",
    "        total_rows += preprocess_and_load(symbol)\n",
    "        # Indicate that processing for one symbol has finished.\n",
    "        progress_queue.put(1)\n",
    "    return total_rows\n",
    "\n",
    "def monitor_progress():\n",
    "    \"\"\"\n",
    "    Monitors and updates the progress of the data loading process.\n",
    "    It counts processed symbols and updates the progress bar.\n",
    "    \"\"\"\n",
    "    processed = 0\n",
    "    while processed < len(files_list):\n",
    "        # Wait until a symbol has been processed.\n",
    "        _ = progress_queue.get()\n",
    "        # Update the progress bar.\n",
    "        pbar.update(1)\n",
    "        processed += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816d78c1affb471faa723ce9cb684f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total rows inserted across processes: 15,261,664 rows\n",
      "Average insertion rate: 194,470 rows/sec\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Main script execution starts.\n",
    "    \n",
    "    # Clear any existing temporary files.\n",
    "    clear_temp_folder()\n",
    "    # Create the required keyspace and table in ScyllaDB.\n",
    "    create_keyspace_and_table()\n",
    "    # List all the financial data files.\n",
    "    files = list_files(CSV_PATH)\n",
    "    # Extract symbol names from file names.\n",
    "    files_list = [element.replace('_full_1min_adjsplit.txt', '') for element in files]\n",
    "    # Sort the file list for consistent processing order.\n",
    "    files_list.sort()\n",
    "    # Initialize a multiprocessing queue to track progress.\n",
    "    progress_queue = mp.Queue()\n",
    "    processes = 5\n",
    "    # (Optional) Limit to the first 10 files for processing.\n",
    "    files_list = files_list[0:20]\n",
    "    # Divide the file list into chunks for parallel processing.\n",
    "    chunks = divide_list_into_chunks(files_list, processes)\n",
    "    # Initialize a progress bar using tqdm.\n",
    "    pbar = tqdm(total=len(files_list))\n",
    "    # Start a separate thread to monitor processing progress.\n",
    "    t = Thread(target=monitor_progress)\n",
    "    t.start()\n",
    "    with mp.Pool(processes=processes) as pool:\n",
    "        results = []\n",
    "        start_time = time.time()\n",
    "        # Process each chunk in parallel using multiprocessing.\n",
    "        for chunk in chunks:\n",
    "            results.append(pool.apply_async(process_chunk, (chunk,)))\n",
    "        # Wait for all processes to finish and get their results.\n",
    "        results = [res.get() for res in results]\n",
    "        end_time = time.time()\n",
    "        # Calculate total processing time.\n",
    "        total_time = end_time - start_time\n",
    "        # Aggregate the number of rows processed across all chunks.\n",
    "        total_rows_aggregated = sum(results)\n",
    "        print(f\"\\nTotal rows inserted across processes: {total_rows_aggregated:,.0f} rows\")\n",
    "        print(f\"Average insertion rate: {total_rows_aggregated / total_time:,.0f} rows/sec\")\n",
    "    # Ensure the progress monitoring thread completes.\n",
    "    t.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader from the Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fetch_minute_data` Function Overview:\n",
    "\n",
    "The `fetch_minute_data` function is designed to pull minute-level financial data from a ScyllaDB (or Cassandra) database. With the assistance of pandas, it provides several functionalities:\n",
    "\n",
    "- **Custom Date Ranges:** Users can set a specific start and end times for fetching the dataset.\n",
    "- **Trading Hours Filter:** Provides an option to retrieve data only from standard trading hours.\n",
    "- **Month-based Bucketing System:** Organizes data for efficient retrieval.\n",
    "- **Asynchronous Data Fetching:** Enables parallel data retrieval across different time intervals.\n",
    "- **Data Rounding:** Users have the option to round numerical values in the dataset to a precision of four decimal places.\n",
    "\n",
    "For a smooth operation, ensure there's an operational ScyllaDB or Cassandra cluster, an appropriate Python environment, and the necessary keyspace and table references.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from cassandra.cluster import Cluster\n",
    "\n",
    "def fetch_minute_data(symbol, start_time, end_time, KEYSPACE, TABLE, trading_hours_only, rounding):\n",
    "    \"\"\"\n",
    "    Fetch minute-level financial data for a specified symbol and timeframe from a ScyllaDB (or Cassandra) database.\n",
    "    \n",
    "    Parameters:\n",
    "    - symbol (str): The ticker symbol to retrieve data for.\n",
    "    - start_time (str): The start of the desired timeframe in the format 'YYYY-MM-DD HH:MM:SS'.\n",
    "    - end_time (str): The end of the desired timeframe in the format 'YYYY-MM-DD HH:MM:SS'.\n",
    "    - KEYSPACE (str): The ScyllaDB/Cassandra keyspace to connect to.\n",
    "    - TABLE (str): The table within the keyspace to query from.\n",
    "    \n",
    "    Returns:\n",
    "    - dataframe (pd.DataFrame): A pandas DataFrame containing the queried financial data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Helper function to generate monthly bucket strings (YYYY-MM) for data partitioning\n",
    "    def generate_monthly_buckets(start_time, end_time):\n",
    "        start_date = datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')\n",
    "        end_date = datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S')\n",
    "        current_date = start_date\n",
    "        buckets = []\n",
    "        \n",
    "        # Loop through each month in the timeframe to generate bucket strings\n",
    "        while current_date <= end_date:\n",
    "            buckets.append(f\"{current_date.year}-{current_date.month:02}\")\n",
    "            \n",
    "            # Advance to the next month\n",
    "            current_date = current_date + timedelta(days=31)\n",
    "            current_date = datetime(current_date.year, current_date.month, 1)\n",
    "        return buckets\n",
    "\n",
    "    # Generate the necessary bucket strings for the desired timeframe\n",
    "    buckets = generate_monthly_buckets(start_time, end_time)\n",
    "    \n",
    "    # List to store dataframes from each bucket\n",
    "    dfs = []\n",
    "    \n",
    "    # Establish a connection to the ScyllaDB/Cassandra cluster and the specific keyspace\n",
    "    cluster = Cluster([SCYLLA_NODE_IP], port=SCYLLA_NODE_PORT)\n",
    "    session = cluster.connect(KEYSPACE)\n",
    "    \n",
    "    # Prepare the query to fetch data\n",
    "    query = f\"SELECT timestamp, open, high, low, close, volume FROM {TABLE} WHERE symbol = ? AND bucket = ? AND \\\"timestamp\\\" >= ? AND \\\"timestamp\\\" <= ?\"\n",
    "    prepared = session.prepare(query)\n",
    "    futures = []\n",
    "\n",
    "    # For each bucket, asynchronously fetch data for the symbol within the given timeframe\n",
    "    for bucket in buckets:\n",
    "        future = session.execute_async(prepared, (symbol, bucket, start_time, end_time))\n",
    "        futures.append(future)\n",
    "\n",
    "    # As queries complete, convert results to pandas dataframes\n",
    "    for future in futures:\n",
    "        rows = future.result()\n",
    "        dfs.append(pd.DataFrame(list(rows)))\n",
    "\n",
    "    # Shutdown the session and cluster connection after fetching data\n",
    "    session.shutdown()\n",
    "    cluster.shutdown()\n",
    "    \n",
    "    # Combine all the dataframes into one dataframe\n",
    "    dataframe = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Set timestamp as the dataframe index and remove the timestamp column\n",
    "    dataframe.index = dataframe['timestamp'].astype('datetime64[ns]') # type: ignore\n",
    "    dataframe.drop('timestamp', inplace=True, axis=1)\n",
    "\n",
    "    if trading_hours_only:\n",
    "        dataframe = dataframe.between_time('09:30:00', '16:00:00')\n",
    "    if rounding:\n",
    "        dataframe = dataframe.round(4)\n",
    "\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1835698 entries, 2005-01-03 09:30:00 to 2023-10-05 16:01:00\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Dtype  \n",
      "---  ------  -----  \n",
      " 0   open    float64\n",
      " 1   high    float64\n",
      " 2   low     float64\n",
      " 3   close   float64\n",
      " 4   volume  float64\n",
      "dtypes: float64(5)\n",
      "memory usage: 84.0 MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                         open      high       low     close    volume\n",
       " timestamp                                                            \n",
       " 2005-01-03 09:30:00   17.2389   17.2389   17.2318   17.2389  173492.0\n",
       " 2005-01-03 09:31:00   17.2389   17.2389   17.2246   17.2246    9646.0\n",
       " 2005-01-03 09:32:00   17.2246   17.2389   17.2246   17.2318   34810.0\n",
       " 2005-01-03 09:33:00   17.2318   17.2389   17.2318   17.2389   11464.0\n",
       " 2005-01-03 09:34:00   17.2389   17.2389   17.2318   17.2318   12302.0\n",
       " ...                       ...       ...       ...       ...       ...\n",
       " 2023-10-05 15:56:00  110.3200  110.3400  110.2800  110.3200    8780.0\n",
       " 2023-10-05 15:57:00  110.3100  110.4350  110.3100  110.3800   13658.0\n",
       " 2023-10-05 15:58:00  110.3750  110.4200  110.3650  110.4200   12518.0\n",
       " 2023-10-05 15:59:00  110.4200  110.4200  110.3100  110.3200   30497.0\n",
       " 2023-10-05 16:01:00  110.3500  110.3500  110.3500  110.3500  195339.0\n",
       " \n",
       " [1835698 rows x 5 columns],\n",
       " None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol = 'A'\n",
    "KEYSPACE = 'financial_data'\n",
    "TABLE = 'test_data_bars_1m_adjsplit'\n",
    "ohlc_df = fetch_minute_data(symbol, '2005-01-01 00:00:00', '2023-10-05 23:59:00', KEYSPACE, TABLE, trading_hours_only=False, rounding=True)\n",
    "ohlc_df, print(ohlc_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           0         1         2         3         4         5\n",
      "0        2005-01-03 09:30:00   17.2389   17.2389   17.2318   17.2389  173492.0\n",
      "1        2005-01-03 09:31:00   17.2389   17.2389   17.2246   17.2246    9646.0\n",
      "2        2005-01-03 09:32:00   17.2246   17.2389   17.2246   17.2318   34810.0\n",
      "3        2005-01-03 09:33:00   17.2318   17.2389   17.2318   17.2389   11464.0\n",
      "4        2005-01-03 09:34:00   17.2389   17.2389   17.2318   17.2318   12302.0\n",
      "...                      ...       ...       ...       ...       ...       ...\n",
      "1835693  2023-10-05 15:56:00  110.3200  110.3400  110.2800  110.3200    8780.0\n",
      "1835694  2023-10-05 15:57:00  110.3100  110.4350  110.3100  110.3800   13658.0\n",
      "1835695  2023-10-05 15:58:00  110.3750  110.4200  110.3650  110.4200   12518.0\n",
      "1835696  2023-10-05 15:59:00  110.4200  110.4200  110.3100  110.3200   30497.0\n",
      "1835697  2023-10-05 16:01:00  110.3500  110.3500  110.3500  110.3500  195339.0\n",
      "\n",
      "[1835698 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(sample_data) # To visually compare with original data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
